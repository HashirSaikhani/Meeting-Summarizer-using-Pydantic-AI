Main Feature: Diverse Data Ingestion and Processing
====================================================

their knowledge base in which format can be, initially knowledge base how to ingest
I expect that someone’s doc will be Google Doc, someone’s Word, someone’s PDF, so I can also transcribe, for us, all things same, we can see what we will get, like some documents and structured comments, and in our system there will be mechanism to upload in multiple documents and convert to co-fax, okay, in this Excel chat cannot happen, yes, in this Excel chat cannot happen, so our backend will do all the work, chat will be like some parsed, and documents will be seen, but one thing in my mind can happen, your thing, that some docs are dropped, but I also, one done step, in it manual may be, so to make it easy, we write multi-sent, that Excel data, send to person, we have sent to client, took it, if Excel support not given, suppose, because in which format will Excel be, exactly its interpretation, why in Excel we said sorry, people maintain in Excel sometimes, and that data for entry fee and it was given by Reki, it happened like that, design complete, so tell that Excel also important, so we say no, whatever raw data you give, you have to give in this form, or this document is Word, you have to fill
We say that Excel, all this is not necessary that we integrate as-is and implement, therefore support we will put in backend, we will say we will do it in re-requirements that yes, this Excel PDF/text/meeting transcript, whatever Google backup, everything, support easily, basically we are registering data, so our chat docs in it, see registered, that general registration of our application, chat docs, they register at high abstract or channel level, not very specific structured semi, I would rather say fully structured, because structured data, you have deterministic, that category is, that not to put in case opinion initially, so we get structured information, whatever structure comes, then we can also put in opinion as we like, we have to put, yes exactly, first we need to make comprehensive type facts, frequent here ask question, which unstructured data is, we also have structured data, in it chunking we do not need to do, just like that, right, when facts go forward, then the answer will come
one is text message, one is audio message, images, some last, there requirement was that when user pressed high, we message, that this pressed route, then with images, etc., designed posters, we can say yes, we posted it, brochures, if here I posted a lot, then keep posting repeatedly, this is exactly, Oscar, if I say, for my one ORD and one ART, then only that should go, it is not that, this is its theoretical, many things, this second phase, no, ok, initial is that system enter will be, what we need to do first, initially our static information will remain, dynamic information which we expect, say, 25 or 40 cases expected, which will be dynamic content, then from DB fetch, ok, and function calling mechanism, tools, those are tools, ok, ok, ok, one or two things done, ingestion full discussion done, just one thought, any ambiguity remains, ok, ok, now whatever on testing per client, whatever prompt comes, it determines intent, some goes to right owner, some I respond from my knowledge, to say hello, its inner logic is inner tool, ok, obviously it is hello response, for your new helpers etc., where knowledge base Q&A comes, if similarity score very low, then it escalates to human, and if repeated attempt fails, it escalates to human, the intent is taken that it has been understood, we have metric timing, yes, we are not satisfied, if escalates to human, now...
We can scrape and fetch latest rates ourselves, latest attempts, because this is scheduled, public schedule, which exam’s schedule is wanted. Another thing, which information we have registered in the FAX, which is also server-side, we need to make protocol for them that they review their respective information individually.
You left out the image, the image file—but images and videos could be used, right? So we have to decide which ones to ignore and which ones to process. Videos—totally ignore them for now—but we still need to view them. That’s a long thing; Adam had transcribed it. We can handle it as text; we can transcribe audio now. First, we handled audio responses, right? Audio cases… in audio cases… audio cases… audio cases… okay, fine. For this, we set rate limits, right? We have a rate limit: user-based rate limits. If we don’t reply within one hour, one minute, or based on some criteria, we can define it. We put rate limits for audio, text, images, video, or any data type. Files can be tagged, etc. We decide as we go. When an image appears, as Osama said, we handle text and audio, but images and video flexibility: we can label video initially but ignore it, extract audio, and treat it as audio. So in our pipeline, for now, we convert video info to text form—then videos are processed.
We keep inherent support in the system but won’t process 3D. We discard unnecessary data. Audio processing: we impose limits. If audio length exceeds a limit, we send a small message telling the user the audio is too long, and we won’t process it. Users should send short messages. Similarly, text can be very long, but the system can only handle a certain limit. Images can be up to 200 files. We have rate limits per message type: text, audio, image, video. We set configurable system limits. 1000 pages, 300 pages, 200 pages—limits exist to prevent overloading. For one bot interaction, we can set a rate limit, e.g., 60 per hour. This is reasonable and configurable.
When we give the system text input, it informs us of all new and existing system data. Our manager, Shams, insists that the bot should not be confused—it should behave clearly, like Mustafa. Security and audit are partially implemented. Dashboard: new messages come in, and the system handles them according to defined rules, ensuring rate limits, message type constraints, and processing priorities are maintained for text, audio, images, and video.
Regarding new information, there should be a review cycle—one or two weeks, configurable. For example, if we collect facts or knowledge, we ingest it and after review, concrete information is approved and added to the system. We may keep raw information for two weeks, and after four weeks, reprocess it to update the text while preserving previous context. Old data is retained but new ingestion should not overlap old data.
If similar information comes in, the system must identify whether it is an already asked question or a new question on a similar topic. For highly relevant facts, we check similarities; if four or five facts match closely, they may be marked as duplicates. A mechanism must be built to handle this dynamically. Scheduled updates will occur in panels but are not fully automatic—user input is needed for dynamic adjustments.
Stack information is stored and auto-updated. Users can configure whether new reviews are done automatically or at initial review level. New information will appear on the dashboard, and automation can be enabled later once confidence levels are satisfactory.
Regarding the dashboard analytics: it should display topics, frequency, matches, and trends in messages and interactions, providing insights on message relevance, new questions, and similarity patterns.
If an email is sent, the information will still be recorded and reflected on the dashboard for insights. We need to determine how much raw information to store.